@article{bengio2003,
    title={A neural probabilistic language model},
    author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
    journal={Journal of machine learning research},
    volume={3},
    number={Feb},
    pages={1137--1155},
    year={2003}
}

@article{word2vec2013,
    title={Efficient Estimation of Word Representations in Vector Space}, 
    author={Tom{\'{a}}s Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
    year={2013},
    month={Sep},
    eprinttype={arXiv},
    eprint={1301.3781},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/1301.3781v3}
}


@article{mikolov2013exploiting,
    title={Exploiting Similarities among Languages for Machine Translation},
    author={Tom{\'{a}}s Mikolov and Quoc V. Le and Ilya Sutskever},
    year={2013},
    month={Sep},
    eprinttype={arXiv},
    eprint={1309.4168},
    primaryClass={cs.CL},
    url={http://arxiv.org/abs/1309.4168}
}


@misc{parrish2017,
    title={Understanding word vectors},
    url={https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469},
    journal={Github},
    author={Parrish, Allison},
    year={2017},
    month={Apr}
}

@misc{colyer2016,
    title={The amazing power of word vectors},
    url={https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/},
    journal={the morning paper},
    author={Colyer, Adrian},
    year={2016},
    month={Apr}
}

@InProceedings{paikens2020,
    author="Paikens, P{\={e}}teris and Znoti{\c{n}}{\v{s}}, Art{\={u}}rs and B{\={a}}rzdi{\c{n}}{\v{s}}, Guntis",
    editor="M{\'e}tais, Elisabeth and Meziane, Farid and Horacek, Helmut and Cimiano, Philipp",
    title="Human-in-the-Loop Conversation Agent for Customer Service",
    booktitle="Natural Language Processing and Information Systems",
    year="2020",
    month={Jun},
    publisher="Springer International Publishing",
    address="Cham",
    pages="277--284",
    url={https://link.springer.com/chapter/10.1007/978-3-030-51310-8_25},
    doi={https://doi.org/10.1007/978-3-030-51310-8_25},
    abstract="This paper describes a prototype system for partial automation of customer service operations of a mobile telecommunications operator with a human-in-the loop conversational agent. The agent consists of an intent detection system for identifying the types of customer requests that it can handle appropriately, a slot filling information extraction system that integrates with the customer service database for a rule-based treatment of the common scenarios, and a template-based language generation system that builds response candidates that can be approved or amended by customer service operators. The main focus of this paper is on the system architecture and machine learning system structure design, and the observations of a limited pilot study performed to evaluate the proposed system on customer messages in Latvian. We also discuss the business requirements and practical application limitations and their influence on the design of the natural language processing components.",
    isbn="978-3-030-51310-8"
}

@misc{mccormick2016,
    title={Word2Vec Tutorial - The Skip-Gram Model},
    url={http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/},
    author={Chris McCormick},
    year={2016},
    month={Apr}
}

@article{fasttext2019,
    title={FastText-Based Intent Detection for Inflected Languages},
    author={Balodis, Kaspars and Deksne, Daiga},
    journal={Information},
    volume={10},
    year={2019},
    number={5:161},
    url={https://www.mdpi.com/2078-2489/10/5/161},
    issn={2078-2489},
    doi={10.3390/info10050161}
}


@incollection{nlp2018,
    title={Chapter 3 - Open-Source Libraries, Application Frameworks, and Workflow Systems for NLP},
    editor={Venkat N. Gudivada and C.R. Rao},
    series={Handbook of Statistics},
    publisher={Elsevier},
    volume={38},
    pages={31-50},
    year={2018},
    booktitle={Computational Analysis and Understanding of Natural Languages: Principles, Methods and Applications},
    issn={0169-7161},
    doi={https://doi.org/10.1016/bs.host.2018.07.007},
    url={https://www.sciencedirect.com/science/article/pii/S0169716118300221},
    author={Venkat N. Gudivada and Kamyar Arbabifard},
    keywords={Natural language processing, Natural language understanding, Information retrieval, Open-source libraries, Workflow systems, Annotated corpora},
    abstract={This chapter provides an annotated listing of various resources for natural language processing research and applications development. Resources include corpora, software libraries and frameworks, and workflow systems.}
}


@inproceedings{atis1990,
    title="The {ATIS} Spoken Language Systems Pilot Corpus",
    author="Hemphill, Charles T. and Godfrey, John J. and Doddington, George R.",
    booktitle="Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley",
    year="1990",
    url="https://catalog.ldc.upenn.edu/docs/LDC93S4B/corpus.html",
}


@inproceedings{de-bruyn-2022,
    title="Machine Translation for Multilingual Intent Detection and Slots Filling",
    author="De bruyn, Maxime and Lotfi, Ehsan and Buhmann, Jeska and Daelemans, Walter",
    booktitle="Proceedings of the Massively Multilingual Natural Language Understanding Workshop (MMNLU-22)",
    month=dec,
    year="2022",
    address="Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher="Association for Computational Linguistics",
    url="https://aclanthology.org/2022.mmnlu-1.8",
    pages="69--82",
    abstract="We expect to interact with home assistants irrespective of our language. However, scaling the Natural Language Understanding pipeline to multiple languages while keeping the same level of accuracy remains a challenge. In this work, we leverage the inherent multilingual aspect of translation models for the task of multilingual intent classification and slot filling. Our experiments reveal that they work equally well with general-purpose multilingual text-to-text models. Furthermore, their accuracy can be further improved by artificially increasing the size of the training set. Unfortunately, increasing the training set also increases the overlap with the test set, leading to overestimating their true capabilities. As a result, we propose two new evaluation methods capable of accounting for an overlap between the training and test set.",
}

@article{firdaus2023,
    title={Multitask learning for multilingual intent detection and slot filling in dialogue systems},
    journal={Information Fusion},
    volume={91},
    pages={299-315},
    year={2023},
    issn={1566-2535},
    doi={https://doi.org/10.1016/j.inffus.2022.09.029},
    url={https://www.sciencedirect.com/science/article/pii/S1566253522001671},
    author={Mauajama Firdaus and Asif Ekbal and Erik Cambria},
    keywords={Multitask learning, Multilingual analysis, Information fusion, Intent detection, Slot filling, Deep learning},
    abstract={Dialogue systems are becoming an ubiquitous presence in our everyday lives having a huge impact on business and society. Spoken language understanding (SLU) is the critical component of every goal-oriented dialogue system or any conversational system. The understanding of the user utterance is crucial for assisting the user in achieving their desired objectives. Future-generation systems need to be able to handle the multilinguality issue. Hence, the development of conversational agents becomes challenging as it needs to understand the different languages along with the semantic meaning of the given utterance. In this work, we propose a multilingual multitask approach to fuse the two primary SLU tasks, namely, intent detection and slot filling for three different languages. While intent detection deals with identifying user’s goal or purpose, slot filling captures the appropriate user utterance information in the form of slots. As both of these tasks are highly correlated, we propose a multitask strategy to tackle these two tasks concurrently. We employ a transformer as a shared sentence encoder for the three languages, i.e., English, Hindi, and Bengali. Experimental results show that the proposed model achieves an improvement for all the languages for both the tasks of SLU. The multi-lingual multi-task (MLMT) framework shows an improvement of more than 2% in case of intent accuracy and 3% for slot F1 score in comparison to the single task models. Also, there is an increase of more than 1 point intent accuracy and 2 points slot F1 score in the MLMT model as opposed to the language specific frameworks.}
}

@inproceedings{jayarao2018,
    author={Jayarao, Pratik and Srivastava, Aman},
    booktitle={2018 International Conference on Electrical, Electronics, Communication, Computer, and Optimization Techniques (ICEECCOT)}, 
    title={Intent Detection for code-mix utterances in task oriented dialogue systems}, 
    year={2018},
    volume={},
    number={},
    pages={583-587},
    doi={10.1109/ICEECCOT43722.2018.9001577}
}


@article{liu2020,
    title={Attention-Informed Mixed-Language Training for Zero-Shot Cross-Lingual Task-Oriented Dialogue Systems},
    volume={34},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/6362},
    DOI={10.1609/aaai.v34i05.6362},
    number={05},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Liu, Zihan and Winata, Genta Indra and Lin, Zhaojiang and Xu, Peng and Fung, Pascale},
    year={2020},
    month={Apr.},
    pages={8433-8440}
}



@inproceedings{conneau2020,
    title="Unsupervised Cross-lingual Representation Learning at Scale",
    author="Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle="Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month=jul,
    year="2020",
    address="Online",
    publisher="Association for Computational Linguistics",
    url="https://aclanthology.org/2020.acl-main.747",
    doi="10.18653/v1/2020.acl-main.747",
    pages="8440--8451",
    abstract="This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}



@book{dangeti2017,
    title="Statistics for Machine Learning: Techniques for exploring supervised, unsupervised, and reinforcement learning models with Python and R ",
    author="Dangeti, Pratap",
    year=2017,
    publisher="Packt Publishing",
    ISBN=9781788295758,
}


@inproceedings{bender2021,
	author={Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
	year={2021},
	isbn={9781450383097},
	publisher={Association for Computing Machinery},
	address={New York, NY, USA},
	url={https://doi.org/10.1145/3442188.3445922},
	doi={10.1145/3442188.3445922},
	abstract={The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	pages={610--623},
	numpages={14},
	location={Virtual Event, Canada},
	series={FAccT '21},
}


@manual{brown-corpus,
	title={Brown Corpus Manual: Manual of information to accompany a Standard Sample of Present-Day American English, for use with digital computers},
	author={Francis, W. Nelson and Kucera, H},
	year={1964},
	publisher={Brown University},
	url={http://icame.uib.no/brown/bcm.html}
}

@misc{wikimedia2020,
	title={Community Insights: Community Insights 2020 Report: Thriving Movement},
	author={Wikimedia Foundation},
	year={2020},
	howpublished ={\url{https://meta.wikimedia.org/wiki/Community_Insights/Community_Insights_2020_Report/Thriving_Movement\#Community_and_Newcomer_Diversity}}
}

@book{1percent,
	title={Structural differentiation in social media: adhocracy, entropy, and the "1 \% effect"},
	author={Britt, Brian C. and Matei, Sorin Adam},
	publisher={Springer},
	isbn={978-3-319-64425-7, 3319644254, 978-3-319-64424-0},
	year={2017},
	series={Lecture notes in social networks},
	edition={},
	volume={},
	url={http://gen.lib.rus.ec/book/index.php?md5=416b9349cbf6cff824e540feb4228cb6}
}


@misc{ngram-viewer,
	title={Google Books Ngram Viewer},
	author={Google Ngram Viewer Team},
	howpublished={\url{https://books.google.com/ngrams/}},
	note={Aplūkots 2023-05-06}
}


@misc{comprised-of,
    title={User:Giraffedata/comprised of},
    author={Bryan Henderson},
    year={2023},
    howpublished ={\url{https://en.wikipedia.org/w/index.php?title=User:Giraffedata/comprised_of&oldid=1151908006}}
}



@inproceedings{braun-2017,
    title = "Evaluating Natural Language Understanding Services for Conversational Question Answering Systems",
    author = "Braun, Daniel  and
      Hernandez Mendez, Adrian  and
      Matthes, Florian  and
      Langen, Manfred",
    booktitle = "Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue",
    month = aug,
    year = "2017",
    address = {Saarbr{\"u}cken, Germany},
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-5522",
    doi = "10.18653/v1/W17-5522",
    pages = "174--185",
    abstract = "Conversational interfaces recently gained a lot of attention. One of the reasons for the current hype is the fact that chatbots (one particularly popular form of conversational interfaces) nowadays can be created without any programming knowledge, thanks to different toolkits and so-called Natural Language Understanding (NLU) services. While these NLU services are already widely used in both, industry and science, so far, they have not been analysed systematically. In this paper, we present a method to evaluate the classification performance of NLU services. Moreover, we present two new corpora, one consisting of annotated questions and one consisting of annotated questions with the corresponding answers. Based on these corpora, we conduct an evaluation of some of the most popular NLU services. Thereby we want to enable both, researchers and companies to make more educated decisions about which service they should use.",
}


@misc{snips-2018,
      title={Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces}, 
      author={Alice Coucke and Alaa Saade and Adrien Ball and Théodore Bluche and Alexandre Caulier and David Leroy and Clément Doumouro and Thibault Gisselbrecht and Francesco Caltagirone and Thibaut Lavril and Maël Primet and Joseph Dureau},
      year={2018},
      eprint={1805.10190},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={\url{https://arxiv.org/abs/1805.10190}},
      note={Aplūkots 2023-05-10}
}


@misc{snips-docs,
  title = {Snips Natural Language Understanding Documentation: Key Concepts \& Data Model},
  author = {Snips},
  howpublished = {\url{https://snips-nlu.readthedocs.io/en/latest/data_model.html\#intent}},
  note={Aplūkots 2023-05-10}
}


@inproceedings{xue2021,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
    Constant, Noah  and
    Roberts, Adam  and
    Kale, Mihir  and
    Al-Rfou, Rami  and
    Siddhant, Aditya  and
    Barua, Aditya  and
    Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.41",
    pages = "483--498",
}

@inproceedings{hu2020,
    title={{XTREME}: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization}, 
    author={Junjie Hu and Sebastian Ruder and Aditya Siddhant and Graham Neubig and Orhan Firat and Melvin Johnson},
    year={2020},
    booktitle = "Proceedings of the 37th International Conference on Machine Learning",
    url = "https://arxiv.org/abs/2003.11080",
}

@inproceedings{devlin2019,
    title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    booktitle={NAACL-HLT 2019: Minneapolis, MN, USA},
    pages={4171--4186},
    year={2019},
    url={https://aclanthology.org/N19-1423.pdf}
}

@inproceedings{li2021,
    title = "{MTOP}: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark",
    author = "Li, Haoran  and
    Arora, Abhinav  and
    Chen, Shuohui  and
    Gupta, Anchit  and
    Gupta, Sonal  and
    Mehdad, Yashar",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    year = "2021",
    url = "https://aclanthology.org/2021.eacl-main.257",
    pages = "2950--2962",
}

@inproceedings{schuster2019,
    title={Cross-Lingual Transfer Learning for Multilingual Task Oriented Dialog},
    author={Sebastian Schuster and Sonal Gupta and Rushin Shah and Mike Lewis},
    booktitle={Proceedings of the 2019 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1},
    pages={3795--3805},
    year={2019},
    url={https://doi.org/10.18653/v1/N19-1380}
}

@inproceedings{masumura2018,
    title={Multi-task and Multi-lingual Joint Learning of Neural Lexical Utterance Classification based on Partially-shared Modeling},
    author={Ryo Masumura and Tomohiro Tanaka and Ryuichiro Higashinaka and
    Hirokazu Masataki and Yushi Aono},
    booktitle={Proceedings of the 27th International Conference on Computational Linguistics},
    pages={1137--1155},
    year={2018},
    url={https://aclanthology.org/C18-1304.pdf}
}

@inproceedings{sekine2004,
    title="Definition, dictionaries and tagger for extended named entity hierarchy",
    author="Satoshi Sekine and Chikashi Nobata",
    booktitle="Proc. Language Resources and Evaluation Conference",
    year="2004"
}

@article{fitzgerald2022,
    title={{MASSIVE}: A 1{M}-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages},
    author={Jack FitzGerald and Christopher Hench and Charith Peris and Scott Mackie and Kay Rottmann and Ana Sanchez and Aaron Nash and Liam Urbach and Vishesh Kakarala and Richa Singh and Swetha Ranganath and Laurie Crist and Misha Britan and Wouter Leeuwis and Gokhan Tur and Prem Natarajan},
    journal={Arxiv.org},
    number={Jun},
    year={2022},
    url={https://arxiv.org/abs/2204.08582}
}


@inproceedings{roy2020,
    title="{A Topic-Aligned Multilingual Corpus of Wikipedia Articles for Studying Information Asymmetry in Low Resource Languages}",
    author="Dwaipayan Roy and Sumit Bhatia and Prateek Jain",
    booktitle="Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)",
    year="2020",
    url="https://aclanthology.org/2020.lrec-1.289.pdf"
}


@inproceedings{tiedemann-2020,
    title = "{OPUS}-{MT} {--} Building open translation services for the World",
    author = {Tiedemann, J{\"o}rg  and Thottingal, Santhosh},
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.61",
    pages = "479--480",
}
