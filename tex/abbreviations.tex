NLP (\textit{natural language processing}) -- dabisko valodu apstrāde.\\
Jēdzientelpa (\textit{word embeddings}) -- vārdu vai frāžu attēlojums daudzdimensionālā vektoru telpā.\\
Word2Vec (\textit{word to vector}) -- jēdzientelpas implementācija, kurā individuālus vārdus aizstāj daudzimensionāli vektori.\\
PCA (\textit{Principal Component Analysis}) -- galveno komponentu analīze.\\
GPT (\textit{Generative Pre-trained Transformer}) – dziļo neironu tīklu modelis, kas spēj producēt tekstu, kas līdzīgs cilvēka rakstītam.\\
Transformeris (\textit{transformer}) -- dziļās mācīšanās modelis ar uzmanības (attention) mehānismu, kas spēj novērtēt ievades daļas nozīmīgumu.\\
Pārpielāgošana (\textit{overfitting}) -- pārmērīga pielāgošanās kādam konkrētai datu kopai, zaudējot spēju ģeneralizēt uz citām datu kopām.\\
Pietrenēšana (\textit{fine-tuning}) -- metode, kurā iepriekš apmācīts modelis tiek pietrenēts jaunam uzdevumam.

XLM (\textit{Cross-lingual Language Model}) -- valodas modelis, kas apmācīts uz daudzvalodu datiem, lai apgūtu jēdzientelpas, ko var pielietot vairākām valodām.\\
Multilingual BERT (\textit{Bidirectional Encoder Representations from Transformers}) --\\
XLM-R (\textit{Cross-lingual Language Model pre-trained with XLM}) --\\
XLM-Roberta (\textit{Robustly Optimized BERT Pretraining Approach}) -- \\
