\begin{python}
from typing import List

import matplotlib.pyplot as plt
import tensorflow as tf
from keras.layers import Dense, Conv1D, Dropout, GlobalMaxPooling1D, \
  MaxPooling1D
from keras.models import Sequential
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, TFBertModel, AutoTokenizer, TFAutoModel


def get_embeddings_tokenizer_model(model_name: str):
  model_name = f"./{model_name}"
  if "roberta" in model_name:
    return AutoTokenizer.from_pretrained(
      model_name), TFAutoModel.from_pretrained(model_name)
  else:
    return BertTokenizer.from_pretrained(
      model_name), TFBertModel.from_pretrained(model_name)


def read_file(path: str) -> List[str]:
  """ Read path and append each line without \n as an element to an array.
  Encoding is specified to correctly read files in Russian.
  Example output:
  ['FindConnection', 'FindConnection', ..., 'FindConnection']
  """
  with open(path, encoding='utf-8') as f:
    array = []
    for line in f:
      array.append(line.rstrip("\n"))
    return array


def get_source_text(
    dataset_type: str, dataset: str,
    source_language: str = None, labels: bool = False,
    machine_translated: bool = False
) -> List[str]:
  """ Wrapper for read_file that provides file path.
  Prompts in all languages are in the same order, therefore they use
  the same label files. So please be careful
  to use the correct argument for labels, as label=True returns labels
  regardless of specified source_language
  Usage examples:
  prompts: read_source_text("test", "et", False)
  labels: read_source_text("test")
  :param dataset_type: "test" or "train"
  :param dataset: "chatbot", "askubuntu" or "webapps"
  :param source_language: "lv", "ru", "et", "lt"
  :param labels: does the file being read contain labels
  :param machine_translated: has the data been machine translated to English?
  :return: array of file contents for specified file
  """
  if labels:
    return read_file(
      f"NLU-datasets\{dataset}\{dataset}_{dataset_type}_ans.txt"
    )
  elif machine_translated:
    return read_file(
      f"machine-translated-datasets\{dataset}_{source_language}_{dataset_type}.txt"
    )
  else:
    return read_file(
      f"NLU-datasets\{dataset}\{source_language}\{dataset}_{dataset_type}_q.txt"
    )


def get_dataset(datasets: dict, dataset: str = "chatbot") -> dict:
  """
  :param datasets: test/train key and languages as values
  :param dataset: "chatbot", "askubuntu" or "webapps"
  :return: dictionary with dataset type, language and optional
     labels and '_en' as keys and list of input data as values
  """
  results = dict()
  for key, value in datasets.items():
    results.update({f"{key}_labels": get_source_text(
      dataset_type=key, dataset=dataset, labels=True)})
    for lang in value:
      results.update({f"{key}_{lang}": get_source_text(
        dataset_type=key, dataset=dataset, source_language=lang)})
      if lang != "en":
        results.update({f"{key}_{lang}_en": get_source_text(
          dataset_type=key, dataset=dataset,
          source_language=lang, machine_translated=True)})
  return results


def split_train_data(x: list, y: list, validation_size: int = 0.2):
  """ Split training set in training and validation
  :param x: data
  :param y: labels
  :param validation_size: what fraction of data to allocate to training?
  :return:
  """
  return train_test_split(x, y, test_size=validation_size, stratify=y,
                          random_state=42)


def split_validation(datasets: dict, data: dict) -> dict:
  """ Split training dataset in training and validation
  :param datasets: dictionary with dataset type as key and list of languages as value
  :param data: dictionary with test/train, language and labels as key and data as values
  :return: updated data dictionary where each train key is split in train and validation
  """
  for key, value in datasets.items():
    if key == "train":
      for lang in value:
        data[f"{key}_{lang}"], \
          data[f"{key}_{lang}_validation"], \
          data[f"{key}_{lang}_labels"], \
          data[f"{key}_{lang}_labels_validation"] = split_train_data(
          data[f"{key}_{lang}"],
          data[f"{key}_labels"])
        if lang != "en":
          data[f"{key}_{lang}_en"], \
            data[f"{key}_{lang}_en_validation"], \
            data[f"{key}_{lang}_en_labels"], \
            data[f"{key}_{lang}_en_labels_validation"] = split_train_data(
            data[f"{key}_{lang}_en"],
            data[f"{key}_labels"])
  return data


def plot_performance(training_data, validation_data, broad_dataset: str,
                     dataset: str, x_label: str = 'accuracy'):
  plt.plot(training_data, label='training')
  plt.plot(validation_data, label='validation')
  ax = plt.gca()
  ax.set_xlabel('epochs')
  ax.set_ylabel(x_label)
  plt.title(f"{dataset} model {x_label}")
  plt.legend(loc="center")
  plt.savefig(f"graphs/{broad_dataset}_{dataset}-{x_label}.png")
  plt.show()


def create_model(sentence_length: int, num_classes: int = 2,
                 hidden_size: int = 768):
  model = Sequential()
  model.add(tf.keras.Input(shape=(sentence_length, hidden_size)))
  model.add(Dense(64, activation='relu'))
  model.add(Conv1D(128, kernel_size=3, activation='relu'))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Conv1D(256, kernel_size=3, activation='relu'))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Conv1D(512, kernel_size=3, activation='relu'))
  model.add(GlobalMaxPooling1D())
  model.add(Dropout(0.1))
  model.add(Dense(256, activation='relu'))
  model.add(Dense(num_classes, activation='softmax'))
  return model


def create_adam_optimizer(
    lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=0,
    epsilon=0, amsgrad=False, clipnorm=1.0):
  # sgd is worse than adam
  return tf.keras.optimizers.Adam(
    learning_rate=lr, beta_1=beta_1,
    beta_2=beta_2, epsilon=epsilon,
    amsgrad=amsgrad, weight_decay=weight_decay, clipnorm=clipnorm
  )


def get_classification_model(
    learning_rate: float, sentence_length: int,
    num_classes: int, clipnorm: float = 1.0
):
  optimizer = create_adam_optimizer(lr=learning_rate, clipnorm=clipnorm)
  classification_model = create_model(
    sentence_length=sentence_length, num_classes=num_classes
  )
  classification_model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
  )
  return classification_model


def training(data, lang: str, learning_rate: float, sentence_length: int,
             batch_size: int, epochs: int,
             model_name: str, broad_dataset: str, num_classes: int = 2):
  train_data = data[f"train_{lang}"]
  train_labels = data[f"train_{lang}_labels"]
  validation_data = data[f"train_{lang}_validation"]
  validation_labels = data[f"train_{lang}_labels_validation"]

  classification_model = get_classification_model(
    learning_rate, sentence_length, num_classes
  )

  history = classification_model.fit(
    train_data,
    y=train_labels,
    batch_size=batch_size,
    epochs=epochs,
    validation_data=(validation_data, validation_labels)
  )

  plot_performance(
    history.history['accuracy'],
    history.history['val_accuracy'],
    broad_dataset=broad_dataset,
    dataset=f"{model_name}_{lang}",
    x_label='accuracy'
  )

  plot_performance(
    history.history['loss'],
    history.history['val_loss'],
    broad_dataset=broad_dataset,
    dataset=f"{model_name}_{lang}",
    x_label='loss'
  )

  return classification_model


def test_classification_model(
    model, data: dict, lang: str,
    batch_size: int) -> float:
  test_data = data[f"test_{lang}"]
  test_labels = data["test_labels"]

  test_loss, test_accuracy = model.evaluate(
    test_data, test_labels,
    batch_size=batch_size)
  print('Test Loss: {:.2f}'.format(test_loss))
  print('Test Accuracy: {:.2f}'.format(test_accuracy))
  return test_accuracy

\end{python}
