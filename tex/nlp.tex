Dabiskā valodas apstrāde (NLP -- \textit{natural language processing}) ir starpdisciplināra datorlingvistikas un mākslīgā intelekta nozare, kas strādā pie tā, lai datori varētu saprast cilvēka dabiskās valodas ievadi. Dabiskās valodas pēc būtības ir sarežģītas, un daudzi NLP uzdevumi ir slikti piemēroti matemātiski precīziem algoritmiskajiem risinājumiem. Palielinoties korpusu (liela apjoma rakstītas vai runātas dabiskās valodas kolekcija) pieejamībai, NLP uzdevumi arvien biežāk un efektīvāk tiek risināti ar mašīnmācīšanās modeļiem \cite{nlp2018}. Dabiskās valodas apstrādei ir liels biznesa potenciāls, jo tas ļauj uzņēmumiem palielināt peļņu samazinot izdevumus, no kuriem lielākais parasti ir darbs. %\% pakalpojumu nozares uzņēmumu izdevumi ir darbs. Var minēt tech layoffs, specifically, Meta’s Year of Efficiency


Viens no svarīgākajiem korpusiem tieši nodomu noteikšanā ir aviokompāniju ceļojumu informācijas sistēmu (ATIS -- \textit{Airline Travel Information Systems}) datu kopa. Tā ir audioierakstu un manuālu transkriptu datu kopa, kas sastāv no cilvēku sarunām ar automatizētām aviolīniju ceļojumu informācijas sistēmām. ATIS datu kopa nodrošina lielu ziņojumu un ar tiem saistīto nodomu skaitu, ko plaši izmanto kā novērtējuma (\textit{benchmark}) datu kopu klasifikatoru apmācībai nodomu noteikšanā \cite{atis1990}.


Lielo teksta korpusu un mašīnmācīšanās modeļa precizitātes vēsture ir cieši saistīta. Agrīnie mašīnmācīšanās algoritmi balstījās uz nelielām manuāli veidotām datu kopām, kas ierobežoja to efektivitāti. Pirmais lielais teksta korpuss bija Brown Corpus, kas tika izdots 1961. gadā un sastāvēja no 1 miljona vārdu angļu teksta no dažādiem avotiem, tas ir mazs apjoms teksta salīdzinot ar mūsdienās pieejamo.

Procesoru jaudas palielināšanās kopā ar datoru un interneta savienojuma pieejamību plašākai sabiedrībai ir radījuši labvēlīgu vidi izveidot un uzglabāt lielu daudzumu digitālo datu, tostarp teksta formā. Lieliem teksta korpusiem ir bijusi izšķiroša loma efektīvu mašīnmācīšanās modeļu izstrādē. Mašīnmācīšanās modeļu efektivitāte ir proporcionāla tiem pieejamo apmācības datu lielumam un kvalitātei. %source

Pirms lielu teksta korpusu pieejamības mašīnmācīšanās modeļi aprobežojās ar mazām un salīdzinoši vienkāršām datu kopām, tādēļ bija grūti sasniegt augstu precizitāti dabiskās valodas apstrādes uzdevumos. Mūsdienās lielos teksta korpusos kā Common Crawl un Wikipedia ir miljardiem vārdu vairākās valodās, kas ļauj modeļiem iemācīties ģenerēt cilvēkiem līdzīgu valodu.

% pievieno info par NLU datasets un common crawl 100

Taču vai visas valodas ir līdzvērtīgi pārstāvētas korpusos? Viegli iztēloties, ka tādi lielie korpusi kā Common Crawl (kopš 2008. gada ievākti petabaiti datu) pārstāv visu Zemes iedzīvotāju uzskatus. Tomēr dažādi faktori traucē visiem rakstīt tekstus internetā, kas vēlāk nokļūst korpusos, piemēram, nabadzība, rakstītneprasme, ierīču un interneta nepieejamība, aizņemtība, karš utml. Tā rezultātā disproporcionāli pārstāvēti gados jaunāki lietotāji no attīstītajām valstīm, piemēram, GPT-2 apmācības dati tika ievākti no Reddit, un pēc Pew Internet Research pētījuma 67\% Reddit lietotāju Amerikas Savienotajās Valstīs ir vīrieši un 64\% vecumā no 18 līdz 29 gadiem. Līdzīgi tikai ~9-15\% no Wikipedia autoriem ir sievietes\cite{bender2021}.

Analoģiski mazāk tekstu tiek rakstīti latviešu valodā, maz-resursu valoda..... pārnest referenci uz tabulu šeit