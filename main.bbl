% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{none/global//global/global}
  \entry{nlp2018}{incollection}{}
    \name{author}{2}{}{%
      {{hash=GVN}{%
         family={Gudivada},
         familyi={G\bibinitperiod},
         given={Venkat\bibnamedelima N.},
         giveni={V\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
      {{hash=AK}{%
         family={Arbabifard},
         familyi={A\bibinitperiod},
         given={Kamyar},
         giveni={K\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=GVN}{%
         family={Gudivada},
         familyi={G\bibinitperiod},
         given={Venkat\bibnamedelima N.},
         giveni={V\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
      {{hash=RC}{%
         family={Rao},
         familyi={R\bibinitperiod},
         given={C.R.},
         giveni={C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Elsevier}%
    }
    \keyw{Natural language processing, Natural language understanding,
  Information retrieval, Open-source libraries, Workflow systems, Annotated
  corpora}
    \strng{namehash}{GVNAK1}
    \strng{fullhash}{GVNAK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This chapter provides an annotated listing of various resources for natural
  language processing research and applications development. Resources include
  corpora, software libraries and frameworks, and workflow systems.%
    }
    \field{booktitle}{Computational Analysis and Understanding of Natural
  Languages: Principles, Methods and Applications}
    \verb{doi}
    \verb https://doi.org/10.1016/bs.host.2018.07.007
    \endverb
    \field{issn}{0169-7161}
    \field{pages}{31\bibrangedash 50}
    \field{series}{Handbook of Statistics}
    \field{title}{Chapter 3 - Open-Source Libraries, Application Frameworks,
  and Workflow Systems for NLP}
    \verb{url}
    \verb https://www.sciencedirect.com/science/article/pii/S0169716118300221
    \endverb
    \field{volume}{38}
    \field{year}{2018}
  \endentry

  \entry{atis1990}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=HCT}{%
         family={Hemphill},
         familyi={H\bibinitperiod},
         given={Charles\bibnamedelima T.},
         giveni={C\bibinitperiod\bibinitdelim T\bibinitperiod},
      }}%
      {{hash=GJJ}{%
         family={Godfrey},
         familyi={G\bibinitperiod},
         given={John\bibnamedelima J.},
         giveni={J\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=DGR}{%
         family={Doddington},
         familyi={D\bibinitperiod},
         given={George\bibnamedelima R.},
         giveni={G\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
    }
    \strng{namehash}{HCTGJJDGR1}
    \strng{fullhash}{HCTGJJDGR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Speech and Natural Language: Proceedings of a Workshop
  Held at Hidden Valley}
    \field{title}{The {ATIS} Spoken Language Systems Pilot Corpus}
    \verb{url}
    \verb https://catalog.ldc.upenn.edu/docs/LDC93S4B/corpus.html
    \endverb
    \field{year}{1990}
  \endentry

  \entry{brown-corpus}{manual}{}
    \name{author}{2}{}{%
      {{hash=FWN}{%
         family={Francis},
         familyi={F\bibinitperiod},
         given={W.\bibnamedelima Nelson},
         giveni={W\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
      {{hash=KH}{%
         family={Kucera},
         familyi={K\bibinitperiod},
         given={H},
         giveni={H},
      }}%
    }
    \list{publisher}{1}{%
      {Brown University}%
    }
    \strng{namehash}{FWNKH1}
    \strng{fullhash}{FWNKH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Brown Corpus Manual: Manual of information to accompany a
  Standard Sample of Present-Day American English, for use with digital
  computers}
    \verb{url}
    \verb http://icame.uib.no/brown/bcm.html
    \endverb
    \field{year}{1964}
  \endentry

  \entry{bender2021}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=BEM}{%
         family={Bender},
         familyi={B\bibinitperiod},
         given={Emily\bibnamedelima M.},
         giveni={E\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=GT}{%
         family={Gebru},
         familyi={G\bibinitperiod},
         given={Timnit},
         giveni={T\bibinitperiod},
      }}%
      {{hash=MMA}{%
         family={McMillan-Major},
         familyi={M\bibinithyphendelim M\bibinitperiod},
         given={Angelina},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Shmitchell},
         familyi={S\bibinitperiod},
         given={Shmargaret},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Association for Computing Machinery}%
    }
    \strng{namehash}{BEM+1}
    \strng{fullhash}{BEMGTMMASS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The past 3 years of work in NLP have been characterized by the development
  and deployment of ever larger language models, especially for English. BERT,
  its variants, GPT-2/3, and others, most recently Switch-C, have pushed the
  boundaries of the possible both through architectural innovations and through
  sheer size. Using these pretrained models and the methodology of fine-tuning
  them for specific tasks, researchers have extended the state of the art on a
  wide array of tasks as measured by leaderboards on specific benchmarks for
  English. In this paper, we take a step back and ask: How big is too big? What
  are the possible risks associated with this technology and what paths are
  available for mitigating those risks? We provide recommendations including
  weighing the environmental and financial costs first, investing resources
  into curating and carefully documenting datasets rather than ingesting
  everything on the web, carrying out pre-development exercises evaluating how
  the planned approach fits into research and development goals and supports
  stakeholder values, and encouraging research directions beyond ever larger
  language models.%
    }
    \field{booktitle}{Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}
    \verb{doi}
    \verb 10.1145/3442188.3445922
    \endverb
    \field{isbn}{9781450383097}
    \field{pages}{610\bibrangedash 623}
    \field{series}{FAccT '21}
    \field{title}{On the Dangers of Stochastic Parrots: Can Language Models Be
  Too Big?}
    \verb{url}
    \verb https://doi.org/10.1145/3442188.3445922
    \endverb
    \list{location}{1}{%
      {Virtual Event, Canada}%
    }
    \field{year}{2021}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{wikimedia2020}{misc}{}
    \name{author}{1}{}{%
      {{hash=FW}{%
         family={Foundation},
         familyi={F\bibinitperiod},
         given={Wikimedia},
         giveni={W\bibinitperiod},
      }}%
    }
    \strng{namehash}{FW1}
    \strng{fullhash}{FW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
  \field{howpublished}{\url{https://meta.wikimedia.org/wiki/Community_Insights/Community_Insights_2020_Report/Thriving_Movement\#Community_and_Newcomer_Diversity}}
    \field{title}{Community Insights: Community Insights 2020 Report: Thriving
  Movement}
    \field{year}{2020}
  \endentry

  \entry{1percent}{book}{}
    \name{author}{2}{}{%
      {{hash=BBC}{%
         family={Britt},
         familyi={B\bibinitperiod},
         given={Brian\bibnamedelima C.},
         giveni={B\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=MSA}{%
         family={Matei},
         familyi={M\bibinitperiod},
         given={Sorin\bibnamedelima Adam},
         giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \strng{namehash}{BBCMSA1}
    \strng{fullhash}{BBCMSA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{isbn}{978-3-319-64425-7, 3319644254, 978-3-319-64424-0}
    \field{series}{Lecture notes in social networks}
    \field{title}{Structural differentiation in social media: adhocracy,
  entropy, and the "1 \% effect"}
    \verb{url}
    \verb http://gen.lib.rus.ec/book/index.php?md5=416b9349cbf6cff824e540feb422
    \verb 8cb6
    \endverb
    \field{year}{2017}
  \endentry

  \entry{ngram-viewer}{misc}{}
    \name{author}{1}{}{%
      {{hash=TGNV}{%
         family={Team},
         familyi={T\bibinitperiod},
         given={Google Ngram\bibnamedelima Viewer},
         giveni={G\bibinitperiod\bibinitdelim N\bibinitperiod\bibinitdelim
  V\bibinitperiod},
      }}%
    }
    \strng{namehash}{TGNV1}
    \strng{fullhash}{TGNV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{howpublished}{\url{https://books.google.com/ngrams/}}
    \field{note}{Aplūkots 2023-05-06}
    \field{title}{Google Books Ngram Viewer}
  \endentry

  \entry{dangeti2017}{book}{}
    \name{author}{1}{}{%
      {{hash=DP}{%
         family={Dangeti},
         familyi={D\bibinitperiod},
         given={Pratap},
         giveni={P\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Packt Publishing}%
    }
    \strng{namehash}{DP1}
    \strng{fullhash}{DP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{isbn}{9781788295758}
    \field{title}{Statistics for Machine Learning: Techniques for exploring
  supervised, unsupervised, and reinforcement learning models with Python and
  R}
    \field{year}{2017}
  \endentry

  \entry{colyer2016}{misc}{}
    \name{author}{1}{}{%
      {{hash=CA}{%
         family={Colyer},
         familyi={C\bibinitperiod},
         given={Adrian},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{CA1}
    \strng{fullhash}{CA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{The amazing power of word vectors}
    \verb{url}
    \verb https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors
    \verb /
    \endverb
    \field{journaltitle}{the morning paper}
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{parrish2017}{misc}{}
    \name{author}{1}{}{%
      {{hash=PA}{%
         family={Parrish},
         familyi={P\bibinitperiod},
         given={Allison},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{PA1}
    \strng{fullhash}{PA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Understanding word vectors}
    \verb{url}
    \verb https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469
    \endverb
    \field{journaltitle}{Github}
    \field{year}{2017}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{word2vec2013}{article}{}
    \name{author}{4}{}{%
      {{hash=MT}{%
         family={Mikolov},
         familyi={M\bibinitperiod},
         given={Tom{\'{a}}s},
         giveni={T\bibinitperiod},
      }}%
      {{hash=CK}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Kai},
         giveni={K\bibinitperiod},
      }}%
      {{hash=CG}{%
         family={Corrado},
         familyi={C\bibinitperiod},
         given={Greg},
         giveni={G\bibinitperiod},
      }}%
      {{hash=DJ}{%
         family={Dean},
         familyi={D\bibinitperiod},
         given={Jeffrey},
         giveni={J\bibinitperiod},
      }}%
    }
    \strng{namehash}{MT+1}
    \strng{fullhash}{MTCKCGDJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{eprint}
    \verb 1301.3781
    \endverb
    \field{title}{Efficient Estimation of Word Representations in Vector Space}
    \verb{url}
    \verb https://arxiv.org/abs/1301.3781v3
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CL}
    \field{year}{2013}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{mccormick2016}{misc}{}
    \name{author}{1}{}{%
      {{hash=MC}{%
         family={McCormick},
         familyi={M\bibinitperiod},
         given={Chris},
         giveni={C\bibinitperiod},
      }}%
    }
    \strng{namehash}{MC1}
    \strng{fullhash}{MC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Word2Vec Tutorial - The Skip-Gram Model}
    \verb{url}
    \verb http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-mod
    \verb el/
    \endverb
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{conneau2020}{inproceedings}{}
    \name{author}{10}{}{%
      {{hash=CA}{%
         family={Conneau},
         familyi={C\bibinitperiod},
         given={Alexis},
         giveni={A\bibinitperiod},
      }}%
      {{hash=KK}{%
         family={Khandelwal},
         familyi={K\bibinitperiod},
         given={Kartikay},
         giveni={K\bibinitperiod},
      }}%
      {{hash=GN}{%
         family={Goyal},
         familyi={G\bibinitperiod},
         given={Naman},
         giveni={N\bibinitperiod},
      }}%
      {{hash=CV}{%
         family={Chaudhary},
         familyi={C\bibinitperiod},
         given={Vishrav},
         giveni={V\bibinitperiod},
      }}%
      {{hash=WG}{%
         family={Wenzek},
         familyi={W\bibinitperiod},
         given={Guillaume},
         giveni={G\bibinitperiod},
      }}%
      {{hash=GF}{%
         family={Guzm{\'a}n},
         familyi={G\bibinitperiod},
         given={Francisco},
         giveni={F\bibinitperiod},
      }}%
      {{hash=GE}{%
         family={Grave},
         familyi={G\bibinitperiod},
         given={Edouard},
         giveni={E\bibinitperiod},
      }}%
      {{hash=OM}{%
         family={Ott},
         familyi={O\bibinitperiod},
         given={Myle},
         giveni={M\bibinitperiod},
      }}%
      {{hash=ZL}{%
         family={Zettlemoyer},
         familyi={Z\bibinitperiod},
         given={Luke},
         giveni={L\bibinitperiod},
      }}%
      {{hash=SV}{%
         family={Stoyanov},
         familyi={S\bibinitperiod},
         given={Veselin},
         giveni={V\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Association for Computational Linguistics}%
    }
    \strng{namehash}{CA+1}
    \strng{fullhash}{CAKKGNCVWGGFGEOMZLSV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This paper shows that pretraining multilingual language models at scale
  leads to significant performance gains for a wide range of cross-lingual
  transfer tasks. We train a Transformer-based masked language model on one
  hundred languages, using more than two terabytes of filtered CommonCrawl
  data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT
  (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average
  accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on
  NER. XLM-R performs particularly well on low-resource languages, improving
  15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM
  models. We also present a detailed empirical analysis of the key factors that
  are required to achieve these gains, including the trade-offs between (1)
  positive transfer and capacity dilution and (2) the performance of high and
  low resource languages at scale. Finally, we show, for the first time, the
  possibility of multilingual modeling without sacrificing per-language
  performance; XLM-R is very competitive with strong monolingual models on the
  GLUE and XNLI benchmarks. We will make our code and models publicly
  available.%
    }
    \field{booktitle}{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}
    \verb{doi}
    \verb 10.18653/v1/2020.acl-main.747
    \endverb
    \field{pages}{8440\bibrangedash 8451}
    \field{title}{Unsupervised Cross-lingual Representation Learning at Scale}
    \verb{url}
    \verb https://aclanthology.org/2020.acl-main.747
    \endverb
    \list{location}{1}{%
      {Online}%
    }
    \field{month}{07}
    \field{year}{2020}
  \endentry

  \entry{fasttext2019}{article}{}
    \name{author}{2}{}{%
      {{hash=BK}{%
         family={Balodis},
         familyi={B\bibinitperiod},
         given={Kaspars},
         giveni={K\bibinitperiod},
      }}%
      {{hash=DD}{%
         family={Deksne},
         familyi={D\bibinitperiod},
         given={Daiga},
         giveni={D\bibinitperiod},
      }}%
    }
    \strng{namehash}{BKDD1}
    \strng{fullhash}{BKDD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb 10.3390/info10050161
    \endverb
    \field{issn}{2078-2489}
    \field{number}{5:161}
    \field{title}{FastText-Based Intent Detection for Inflected Languages}
    \verb{url}
    \verb https://www.mdpi.com/2078-2489/10/5/161
    \endverb
    \field{volume}{10}
    \field{journaltitle}{Information}
    \field{year}{2019}
  \endentry

  \entry{paikens2020}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=PP}{%
         family={Paikens},
         familyi={P\bibinitperiod},
         given={P{\={e}}teris},
         giveni={P\bibinitperiod},
      }}%
      {{hash=ZA}{%
         family={Znoti{\c{n}}{\v{s}}},
         familyi={Z\bibinitperiod},
         given={Art{\={u}}rs},
         giveni={A\bibinitperiod},
      }}%
      {{hash=BG}{%
         family={B{\={a}}rzdi{\c{n}}{\v{s}}},
         familyi={B\bibinitperiod},
         given={Guntis},
         giveni={G\bibinitperiod},
      }}%
    }
    \name{editor}{4}{}{%
      {{hash=ME}{%
         family={M{\'e}tais},
         familyi={M\bibinitperiod},
         given={Elisabeth},
         giveni={E\bibinitperiod},
      }}%
      {{hash=MF}{%
         family={Meziane},
         familyi={M\bibinitperiod},
         given={Farid},
         giveni={F\bibinitperiod},
      }}%
      {{hash=HH}{%
         family={Horacek},
         familyi={H\bibinitperiod},
         given={Helmut},
         giveni={H\bibinitperiod},
      }}%
      {{hash=CP}{%
         family={Cimiano},
         familyi={C\bibinitperiod},
         given={Philipp},
         giveni={P\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer International Publishing}%
    }
    \strng{namehash}{PPZABG1}
    \strng{fullhash}{PPZABG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This paper describes a prototype system for partial automation of customer
  service operations of a mobile telecommunications operator with a
  human-in-the loop conversational agent. The agent consists of an intent
  detection system for identifying the types of customer requests that it can
  handle appropriately, a slot filling information extraction system that
  integrates with the customer service database for a rule-based treatment of
  the common scenarios, and a template-based language generation system that
  builds response candidates that can be approved or amended by customer
  service operators. The main focus of this paper is on the system architecture
  and machine learning system structure design, and the observations of a
  limited pilot study performed to evaluate the proposed system on customer
  messages in Latvian. We also discuss the business requirements and practical
  application limitations and their influence on the design of the natural
  language processing components.%
    }
    \field{booktitle}{Natural Language Processing and Information Systems}
    \verb{doi}
    \verb https://doi.org/10.1007/978-3-030-51310-8_25
    \endverb
    \field{isbn}{978-3-030-51310-8}
    \field{pages}{277\bibrangedash 284}
    \field{title}{Human-in-the-Loop Conversation Agent for Customer Service}
    \verb{url}
    \verb https://link.springer.com/chapter/10.1007/978-3-030-51310-8_25
    \endverb
    \list{location}{1}{%
      {Cham}%
    }
    \field{year}{2020}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{de-bruyn-2022}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=DbM}{%
         family={De\bibnamedelima bruyn},
         familyi={D\bibinitperiod\bibinitdelim b\bibinitperiod},
         given={Maxime},
         giveni={M\bibinitperiod},
      }}%
      {{hash=LE}{%
         family={Lotfi},
         familyi={L\bibinitperiod},
         given={Ehsan},
         giveni={E\bibinitperiod},
      }}%
      {{hash=BJ}{%
         family={Buhmann},
         familyi={B\bibinitperiod},
         given={Jeska},
         giveni={J\bibinitperiod},
      }}%
      {{hash=DW}{%
         family={Daelemans},
         familyi={D\bibinitperiod},
         given={Walter},
         giveni={W\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Association for Computational Linguistics}%
    }
    \strng{namehash}{DbM+1}
    \strng{fullhash}{DbMLEBJDW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    We expect to interact with home assistants irrespective of our language.
  However, scaling the Natural Language Understanding pipeline to multiple
  languages while keeping the same level of accuracy remains a challenge. In
  this work, we leverage the inherent multilingual aspect of translation models
  for the task of multilingual intent classification and slot filling. Our
  experiments reveal that they work equally well with general-purpose
  multilingual text-to-text models. Furthermore, their accuracy can be further
  improved by artificially increasing the size of the training set.
  Unfortunately, increasing the training set also increases the overlap with
  the test set, leading to overestimating their true capabilities. As a result,
  we propose two new evaluation methods capable of accounting for an overlap
  between the training and test set.%
    }
    \field{booktitle}{Proceedings of the Massively Multilingual Natural
  Language Understanding Workshop (MMNLU-22)}
    \field{pages}{69\bibrangedash 82}
    \field{title}{Machine Translation for Multilingual Intent Detection and
  Slots Filling}
    \verb{url}
    \verb https://aclanthology.org/2022.mmnlu-1.8
    \endverb
    \list{location}{1}{%
      {Abu Dhabi, United Arab Emirates (Hybrid)}%
    }
    \field{month}{12}
    \field{year}{2022}
  \endentry

  \entry{firdaus2023}{article}{}
    \name{author}{3}{}{%
      {{hash=FM}{%
         family={Firdaus},
         familyi={F\bibinitperiod},
         given={Mauajama},
         giveni={M\bibinitperiod},
      }}%
      {{hash=EA}{%
         family={Ekbal},
         familyi={E\bibinitperiod},
         given={Asif},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CE}{%
         family={Cambria},
         familyi={C\bibinitperiod},
         given={Erik},
         giveni={E\bibinitperiod},
      }}%
    }
    \keyw{Multitask learning, Multilingual analysis, Information fusion, Intent
  detection, Slot filling, Deep learning}
    \strng{namehash}{FMEACE1}
    \strng{fullhash}{FMEACE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Dialogue systems are becoming an ubiquitous presence in our everyday lives
  having a huge impact on business and society. Spoken language understanding
  (SLU) is the critical component of every goal-oriented dialogue system or any
  conversational system. The understanding of the user utterance is crucial for
  assisting the user in achieving their desired objectives. Future-generation
  systems need to be able to handle the multilinguality issue. Hence, the
  development of conversational agents becomes challenging as it needs to
  understand the different languages along with the semantic meaning of the
  given utterance. In this work, we propose a multilingual multitask approach
  to fuse the two primary SLU tasks, namely, intent detection and slot filling
  for three different languages. While intent detection deals with identifying
  user’s goal or purpose, slot filling captures the appropriate user
  utterance information in the form of slots. As both of these tasks are highly
  correlated, we propose a multitask strategy to tackle these two tasks
  concurrently. We employ a transformer as a shared sentence encoder for the
  three languages, i.e., English, Hindi, and Bengali. Experimental results show
  that the proposed model achieves an improvement for all the languages for
  both the tasks of SLU. The multi-lingual multi-task (MLMT) framework shows an
  improvement of more than 2% in case of intent accuracy and 3% for slot F1
  score in comparison to the single task models. Also, there is an increase of
  more than 1 point intent accuracy and 2 points slot F1 score in the MLMT
  model as opposed to the language specific frameworks.%
    }
    \verb{doi}
    \verb https://doi.org/10.1016/j.inffus.2022.09.029
    \endverb
    \field{issn}{1566-2535}
    \field{pages}{299\bibrangedash 315}
    \field{title}{Multitask learning for multilingual intent detection and slot
  filling in dialogue systems}
    \verb{url}
    \verb https://www.sciencedirect.com/science/article/pii/S1566253522001671
    \endverb
    \field{volume}{91}
    \field{journaltitle}{Information Fusion}
    \field{year}{2023}
  \endentry

  \entry{liu2020}{article}{}
    \name{author}{5}{}{%
      {{hash=LZ}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Zihan},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=WGI}{%
         family={Winata},
         familyi={W\bibinitperiod},
         given={Genta\bibnamedelima Indra},
         giveni={G\bibinitperiod\bibinitdelim I\bibinitperiod},
      }}%
      {{hash=LZ}{%
         family={Lin},
         familyi={L\bibinitperiod},
         given={Zhaojiang},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=XP}{%
         family={Xu},
         familyi={X\bibinitperiod},
         given={Peng},
         giveni={P\bibinitperiod},
      }}%
      {{hash=FP}{%
         family={Fung},
         familyi={F\bibinitperiod},
         given={Pascale},
         giveni={P\bibinitperiod},
      }}%
    }
    \strng{namehash}{LZ+1}
    \strng{fullhash}{LZWGILZXPFP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb 10.1609/aaai.v34i05.6362
    \endverb
    \field{number}{05}
    \field{pages}{8433\bibrangedash 8440}
    \field{title}{Attention-Informed Mixed-Language Training for Zero-Shot
  Cross-Lingual Task-Oriented Dialogue Systems}
    \verb{url}
    \verb https://ojs.aaai.org/index.php/AAAI/article/view/6362
    \endverb
    \field{volume}{34}
    \field{journaltitle}{Proceedings of the AAAI Conference on Artificial
  Intelligence}
    \field{year}{2020}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{jayarao2018}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=JP}{%
         family={Jayarao},
         familyi={J\bibinitperiod},
         given={Pratik},
         giveni={P\bibinitperiod},
      }}%
      {{hash=SA}{%
         family={Srivastava},
         familyi={S\bibinitperiod},
         given={Aman},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{JPSA1}
    \strng{fullhash}{JPSA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{2018 International Conference on Electrical, Electronics,
  Communication, Computer, and Optimization Techniques (ICEECCOT)}
    \verb{doi}
    \verb 10.1109/ICEECCOT43722.2018.9001577
    \endverb
    \field{pages}{583\bibrangedash 587}
    \field{title}{Intent Detection for code-mix utterances in task oriented
  dialogue systems}
    \field{year}{2018}
  \endentry
\enddatalist
\endinput
